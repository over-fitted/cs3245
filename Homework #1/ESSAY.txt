1. I assume that tokenisation in this context is simply to remove the spaces and punctuation, where characters from different words can be put in the same ngram without spaces. This does not change the capitalisation of characters. I expect this would actually reduce the accuracy of the models. By including tokenisation, we add some context of whether the characters actually belong to the same word or are part of different words. By losing this context, the model may think words formed by the concatenation of the last characters of word A and the first characters of word B are actual words when they are in fact not.

2. More data for all LMs would result in more accurate results as there would be a higher chance of a higher proportion of ngrams actually being seen and accounted for instead of just skipping them. However, this may also increase the chance of others statements being mislabelled as being of the languages as there would be a higher chance of coincidental addition of the ngrams despite not being part of words that actually belong to these languages. If a massive and diverse dataset is added for just indonesian, there would be a higher chance that these others inputs would be mislabelled as indonesian, and there would be a higher chance that ambiguous inputs that can be seen as either malaysian or indonesian be labelled as malaysian. This is because each ngram is likely to have a smaller share of the corpus in the case of indonesian, resulted in a deflated probability.

3. Stripping out punctuation and numbers would likely increase the accuracy of the model. Punctuation and numbers do not belong to any one language, and add an unnecessary bias to languages that contain these punctuation and numbers in the training data. Further, particular the context provided by punctuation is largely already handled by the inclusion of spaces, and ngrams are too small to provide enough context for the sentence breaks to be useful. For similar reason regarding sentence breaks, removal of uppercase is likely to be useful as they are only used to indicate proper nouns and beginning of sentences, not specific to any one language.

4. Using smaller ngram sizes is likely less useful as they would provide even less context about the words they represent which are the fundamental building blocks of languages. Instead, using word-based ngrams is likely more useful by keeping this context.